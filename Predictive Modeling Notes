Predictive Modeling Notes
---------

####Week 1 
https://www.otexts.org/fpp/2/1
HA - Chapter 2

Always plot the data first

```
library(fpp)

plot(melsyd[,"Economy.Class"], 
  main="Economy class passengers: Melbourne-Sydney",
  xlab="Year",ylab="Thousands")

```

time series patterns - there are two words used to describe patterns trend and seasonal

A trend exists when there is a long-term increase or decrease in the data. There is a trend in the antidiabetic drug sales data shown above

A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. The monthly sales of antidiabetic drugs above shows seasonality partly induced by the change in cost of the drugs at the end of the calendar year.

A cycle occurs when the data exhibit rises and falls that are not of a fixed period. These fluctuations are usually due to economic conditions and are often related to the "business cycle". The economy class passenger data above showed some indications of cyclic effects.

Seasonal patterns have a fixed and known length, while cyclic patterns have variable and unknown length.

QUESTION: trend vs seasonale plot?

scatter plots - useful for cross sectional data

Numerical data summaries

Univariante statistics
For a single data set, the most widely used statistics are the average and median.

average is also called the sample mean.

median is the middle observation when the data are placed in order.

Standard deviation: describe distribution of the data

Bivariate Statistics
The most commonly used bivariate statistic is the correlation coefficient 

autocorrelation measures the linear relationship between lagged values of a time series

Simple Forecasting Methods

Average Method: forecasts of all future values are equal to the mean of the historical data - BOTH TIME SERIES AND CROSS SECTIONAL

meanf(y, h) 

Naïve method: All forecasts are simply set to be the value of the last observation. Works well with financial data. Can only be used with time series data

naive(y, h)
rwf(y, h) # Alternative

Seasonal naïve method: each forecast to be equal to the last observed value from the same season of the year (e.g., the same month of the previous year)

snaive(y, h)

Drift Method: A variation on the naïve method is to allow the forecasts to increase or decrease over time, where the amount of change over time (called the drift) is set to be the average change seen in the historical data.

rwf(y, h, drift=TRUE)

Transformations and Adjustments
Adjusting the historical data can often lead to a simpler forecasting model

Mathematical Transformations: If the data show variation that increases or decreases with the level of the series, then a transformation can be useful.

Log transformations
power transformations (square root, cube root)
"Box-Cox transformations": The logarithm in a Box-Cox transformation is always a natural logarithm (i.e., to base e). So if λ=0, natural logarithms are used, but if λ≠0, a power transformation is used followed by some simple scaling.

Calendar Adjustments: Some variation seen in seasonal data may be due to simple calendar effects. In such cases, it is usually much easier to remove the variation before fitting a forecasting model.

Population Adjustments: Any data that are affected by population changes can be adjusted to give per-capita data. That is, consider the data per person (or per thousand people, or per million people) rather than the total.

Inflation Adjustments: Data that are affected by the value of money are best adjusted before modelling. 

Evaluating Forecast Accuracy 

Accuracy measures that are based on ei are therefore scale-dependent and cannot be used to make comparisons between series that are on different scales.

Scale Dependent Errors: The two most commonly used scale-dependent measures are based on the absolute errors or squared errors

Percentage Errors: Percentage errors have the advantage of being scale-independent, and so are frequently used to compare forecast performance between different data sets. The most commonly used measure is Mean absolute percentage error

Scaled Errors: mean absolute scaled error. an alternative to using percentage errors when comparing forecast accuracy across series on different scales

Residual diagnostics: A residual in forecasting is the difference between an observed value and its forecast based on other observations: ei=yi−y^i.

A good forecasting method will yield residuals with the following properties:
1)The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.
2)The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased.


Prediction Intervals: prediction interval gives an interval within which we expect yi to lie with a specified probability.

KJ - Chapter 3 and 4

Data Preprocessing (chapter 3)

Data Transformation for Individual Predictors

Centering and Scaling: center scale the predictor variables. To center a predictor variable, the average predictor value is subtracted from all the values. As a result of centering, the predictor has a zero mean. To scale the data, each value of the predictor variable is divided by its standard deviation. Scaling the data coerce the values to have a common standard deviation of one. 

used to improve the numerical stability of some calculations.

Box and Cox

Data Reduction Techniques: Data reduction techniques are another class of predictor transformations. These methods reduce the data by generating a smaller set of predictors that seek to capture a majority of the information in the original variables.


####Week 2

HA Chapter 4

The simple linear model

y=β0+β1x+ε

The parameters β0 and β1 determine the intercept and the slope of the line

We assume that these errors:

1)have mean zero; otherwise the forecasts will be systematically biased.
2)are not autocorrelated; otherwise the forecasts will be inefficient as there is more information to be exploited in the data.
3)are unrelated to the predictor variable; otherwise there would be more information that should be included in the systematic part of the model.

The least squares principle provides a way of choosing β0 and β1 effectively by minimizing the sum of the squared errors. That is, we choose the values of β0 and β1 that minimize

∑i=1Nε2i=∑i=1N(yi−β0−β1xi)2.

β^0=y¯−β^1x¯,
where x¯ is the average of the x observations and y¯ is the average of the y observations.

The difference between the observed y values and the corresponding fitted values are the “residuals”:

ei=yi−y^i=yi−β^0−β^1xi.
The residuals have some useful properties including the following two:

∑i=1Nei=0and∑i=1Nxiei=0.

the average of the residuals is zero, and that the correlation between the residuals and the observations for predictor variable is also zero.

r measures the strength and the direction (positive or negative) of the linear relationship between the two variables. The stronger the linear relationship, the closer the observed data points will cluster around a straight line.

correlation and regression are strongly linked. The advantage of a regression model over correlation is that it asserts a predictive relationship between the two variables (x predicts y) and quantifies this in a way that is useful for forecasting.

residual is the unpredictable random component of each observation and is defined as
ei=yi−y^i,

way to summarize how well a linear regression model fits the data is via the coefficient of determination or R2. This can be calculated as the square of the correlation between the observed y values and the predicted y^ values. 

Another measure of how well the model has fitted the data is the standard deviation of the residuals, which is often known as the “standard error of the regression” 

transforming variables y and/or x and then estimating a regression model using the transformed variables is the simplest way of obtaining a non-linear specification.

Scenario Based Forecasting: forecaster assumes possible scenarios for the predictor variable that are of interest.

Ex ante forecasts are those that are made using only the information that is available in advance. For example, ex ante forecasts of consumption for the four quarters in 2011 should only use information that was available before 2011.

Ex post forecasts are those that are made using later information on the predictors. For example, ex post forecasts of consumption for each of the 2011 quarters may use the actual observations of income for each of these quarters, once these have been observed. These are not genuine forecasts, but are useful for studying the behaviour of forecasting models. 

Linear Trend: regression we can model and forecast the trend in time series data by including t=1,…,T, as a predictor variable:
yt=β0+β1t+εt.

fit.ex4 <- tslm(austa ~ trend)
f <- forecast(fit.ex4, h=5,level=c(80,95))
plot(f, ylab="International tourist arrivals to Australia (millions)",
  xlab="t")
lines(fitted(fit.ex4),col="blue")
summary(fit.ex4)

Residual Autocorrelation: time series data it is highly likely that the value of a variable observed in the current time period will be influenced by its value in the previous period, or even the period before that

it is very common to find autocorrelation in the residuals

Spurious Regression: time series data are “non-stationary”; that is, the values of the time series do not fluctuate around a constant mean or with a constant variance


HA - Chapter 5

The general form of a multiple regression is
yi=β0+β1x1,i+β2x2,i+⋯+βkxk,i+ei,
where yi is the variable to be forecast and x1,i,…,xk,i are the k predictor variables. Each of the predictor variables must be numerical. The coefficients β1,…,βk measure the effect of each predictor after taking account of the effect of all other predictors in the model. Thus, the coefficients measure the marginal effects of the predictor variables.

As for simple linear regression, when forecasting we require the following assumptions for the errors (e1,…,eN):

the errors have mean zero;
the errors are uncorrelated with each other;
the errors are uncorrelated with each predictor xj,i.

As with simple regression, the residuals have zero mean and are uncorrelated with any of the predictors.

Trend: Not recommended that quadratic or higher order trends are used in forecasting. When they are extrapolated, the resulting forecasts are often very unrealistic. Better approach is to use a piecewise linear trend which bends at some time. If the trend bends at time τ

Ex post Ex Ante forecasting: ex ante forecasts are those that are made using only the information that is available in advance, while ex post forecasts are those that are made using later information on the predictors.

Normally, we cannot use future values of the predictor variables when producing ex ante forecasts because their values will not be known in advance. However, the special predictors introduced in this section are all known in advance, as they are based on calendar variables (e.g., seasonal dummy variables or public holiday indicators) or deterministic functions of time.


When there are many possible predictors, use a measure of predictive accuracy. Five such measures are below:

Adjusted R-squared or r-bar-squared: Maximizing R¯2 is equivalent to minimizing the following estimate of the variance of the forecast errors:
σ^2=SSEN−k−1.
Maximizing R¯2 works quite well as a method of selecting predictors, although it does tend to err on the side of selecting too many predictors.

Cross-validation:

leave-one-out cross-validation for regression can be carried out using the following steps.

Remove observation i from the data set, and fit the model using the remaining data. Then compute the error (e∗i=yi−y^i) for the omitted observation. (This is not the same as the residual because the ith observation was not used in estimating the value of y^i.)
Repeat step 1 for i=1,…,N.
Compute the MSE from e∗1,…,e∗N. We shall call this the CV.

Another test of autocorrelation that is designed to take account of the regression model is the Durbin-Watson test. It is used to test the hypothesis that there is no lag one autocorrelation in the residuals. If there is no autocorrelation, the Durbin-Watson distribution is symmetric around 2.


KJ Chapter 5



KJ Chapter 6


KJ Chapter 7

Neural Networks: neural networks have a tendency to over-fit the relationship between the predictors and the response due to the large number of regression coeffi- cients. To combat this issue, several different approaches have been proposed.

1) iterative algorithms for solving for the regression equations can be prematurely halted 

2) Another approach to moderating over-fitting is to use weight decay, a pe- nalization method to regularize the model similar to ridge regression discussed in the last chapter. Here, we add a penalty for large regression coefficients so that any large value must have a significant effect on the model errors to be tolerated.

KJ Chapter 8

Regression Trees and Rule-Based Models

Tree-based models consist of one or more nested if-then statements for the predictors that partition the data. Within these partitions, a model is used to predict the outcome. 

Tree-based and rule-based models are popular modeling tools for a num- ber of reasons. First, they generate a set of conditions that are highly inter- pretable and are easy to implement. Because of the logic of their construction, they can effectively handle many types of predictors (sparse, skewed, contin- uous, categorical, etc.) without the need to pre-process them. In addition, these models do not require the user to specify the form of the predictors’ re- lationship to the response like, for example, a linear regression model requires. Furthermore, these models can effectively handle missing data and implicitly conduct feature selection, characteristics that are desirable for many real-life modeling problems.

Two well-known weaknesses are (1) model instability (i.e., slight changes in the data can drastically change the structure of the tree or rules and, hence, the interpretation) and (2) less-than-optimal predictive perfor- mance.



Week 5
HA - 6.1, 6.2, 7.4, 7.5
HA Chapter 6

3 types of time series patterns:

Trend
A trend exists when there is a long-term increase or decrease in the data. It does not have to be linear. Sometimes we will refer to a trend “changing direction” when it might go from an increasing trend to a decreasing trend.
Seasonal
A seasonal pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week). Seasonality is always of a fixed and known period.
Cyclic
A cyclic pattern exists when data exhibit rises and falls that are not of fixed period. The duration of these fluctuations is usually of at least 2 years.

 the estimate of the trend-cycle at time t is obtained by averaging values of the time series within k periods of t


