---
title: "Week 2 Homework"
author: "Ben Arancibia"
date: "June 20, 2015"
output: pdf_document
---

#### HA 4.1

Electricity consumption was recorded for a small town on 12 randomly chosen days. The following maximum temperatures (degrees Celsius) and consumption (megawatt-hours) were recorded for each day. 

```{r, comment=FALSE,warning=FALSE,message=FALSE}
library(fpp)
```
```{r}
knitr::kable(econsumption)
```

a) Plot the data and find the regression model for Mwh with temperature as an explanatory variable. Why is there a negative relationship?

```{r}
plot(Mwh ~ temp, data=econsumption)
fit  <- lm(Mwh ~ temp, data=econsumption)
summary(fit)
```

It seems that there is a negative relationship because a simple linear model is not appropriate. A non-linear model will be necessary for the data.

b) Produce a residual plot. Is the model adequate? Are there any outliers or influential observations?

```{r}
plot(residuals(fit)~temp, data=econsumption)
```

The model appears to be adequate, there is no apparent pattern. The only thing that might cause an issues is 1 outlier at temperature 23.4.


c) Use the model to predict the electricity consumption that you would expect for a day with maximum temperature 10 and a day with maximum temperature 35. Do you believe these predictions?

```{r}
fcast <- forecast(fit, newdata=data.frame(temp=c(10,35)))
plot(fcast)
```

Based on the predictive intervals and the nearby data points, the predictions are believable.

d) Give prediction intervals for your forecasts. The following R code will get you started:

```{r}
summary(fit)
forecast(fit, newdata=data.frame(temp=c(60)))
```

####HA 5.2

The data below (data set texasgas) shows the demand for natural gas and the price of natural gas for 20 towns in Texas in 1969.

```{r}
knitr::kable(texasgas)
```

a) Do a scatterplot of consumption against price.

```{r}
plot(consumption ~ price, data=texasgas,main="Consumption vs Price", xlab="price ", ylab="consumption ")
```

b) The slope of the fitted line should change with P, because this is not a linear relationship. It is a non-linear relationship since it is a negatative relationship.

c) Fit the three models and find the coefficients, and residual variance in each case.

Model 1 - Log 
```{r}
fit1 <- lm(consumption~price, data=texasgas)
plot(fitted(fit1), texasgas$price,  main="Consumption vs Price",ylab="consumption", xlab="Predicted Price")

summary(fit1)
```

Model 2 - Piecewise Linear
```{r}
pricep <- pmax(texasgas$price-60,0)
fit2 <- lm(consumption~price+pricep,data=texasgas)
x <- 30:100; z <- pmax(x-60,0)
fcast2 <- forecast(fit2, newdata=data.frame(price=x,pricep=z))
plot(jitter(consumption)~jitter(price), data=texasgas, main="Consumption vs Price" )
lines(x, fcast2$mean,col="red")

summary(fit2)
```

Model 3 - Polynomial nonlinear regression

```{r}
fit3 <- lm(consumption ~ price + I(price^2), texasgas)
fcast3 <- forecast(fit3, newdata=data.frame(price=x,pricep=z))
plot(jitter(consumption)~jitter(price), data=texasgas, main="Consumption vs Price" )
lines(x, fcast3$mean,col="red")

summary(fit3)
```


d) For each model, find the value of R2 and AIC, and produce a residual plot. Comment on the adequacy of the three models.
```{r}
CV(fit1)
plot(fitted(fit1), residuals(fit1), xlab="price", ylab="Residuals")
```

```{r}
CV(fit2)
plot(fitted(fit2), residuals(fit2), xlab="price", ylab="Residuals")
```

```{r}
CV(fit3)
plot(fitted(fit3), residuals(fit3), xlab="price", ylab="Residuals")
```

The piecewise linear and polynomial nonlinear regression models have the "best" adjusted R-square values and AIC. It seems that the piecewise linear model does the best explaining the relation of the two variables.

e) For prices 40, 60, 80, 100, and 120 cents per 1,000 cubic feet, compute the forecasted per capita demand using the best model of the three above.

```{r}
x <- seq(40,120, 20); z <- pmax(x-60,0)
predicted <- predict(fit2,newdata=data.frame(price=x,pricep=z),interval="prediction")
matplot(x,predicted,type="l",lty=1,lwd=1.5,col=c("thistle","orange","orange"), xlab="price", ylab="predicted demand", main="Forecasted Per Capita Demand" )
```

f) Compute 95% prediction intervals. Make a graph of these prediction intervals and discuss their interpretation.
####Model 1
```{r}
confint(fit1,level=0.95)
```

####Model 2
```{r}
confint(fit2,level=0.95)
```

####Model 3
```{r}
confint(fit3,level=0.95)
```

g) What is the correlation between P and P2? Does this suggest any general problem to be considered in dealing with polynomial regressions---especially of higher orders?

```{r}
cor(texasgas$price,I(texasgas$price^2))
```

Since it is almost 1, this suggests an issue of multicollinearity. This means it can be difficult to estimate the regression model, uncertainty associated with individual regression coeffiecients will be large, and forecasts will be unreliable if the values of the future predictors are outside the range of the historical values of the predictors.

####KJ 6.2

a) Load the necessary data and take a look at it
```{r}
library(AppliedPredictiveModeling)
library(caret)
library(pls)
library(elasticnet)
data(permeability)

hist(permeability, xlab="Permeability")
```

One thing to notice is the data is skewed, so log-transform the data.

```{r}
hist(log10(permeability), xlab="log10(Permeability)")
```

b) The fingerprint predictors indicate the presence or absence of substructures of a molecule and are often sparse meaning that relatively few of the molecules contain each substructure. Filter out the predictors that have low frequencies using the nearZeroVar function from the caret package. How many predictors are left for modeling?

First pre-process the fingerprint predictors remove near-zero variance fingerprint predictors.

```{r}
nzvfingerprints <- nearZeroVar(fingerprints)
nonzvfingerprints <- fingerprints[,-nzvfingerprints]
length(nzvfingerprints)
ncol(nonzvfingerprints)
```

There are 719 near-zero variance fingerprints, which means there is 388 left for modeling. This is a significant reduction from the original and indicates that many of the fingerprints are describing unique features of very small subsets of molecules.

c) Split the data into a training and a test set, pre-process the data, and tune a PLS model. How many latent variables are optimal and what is the corresponding resampled estimate of R2?

```{r}
set.seed(614)
trainingRows <- createDataPartition(permeability, 
                                    p = 0.75, 
                                    list = FALSE)

trainFingerprints <- nonzvfingerprints[trainingRows,]
trainPermeability <- permeability[trainingRows,]

testFingerprints <- nonzvfingerprints[-trainingRows,]
testPermeability <- permeability[-trainingRows,]
```

```{r}
set.seed(614)
ctrl <- trainControl(method = "LGOCV")

plsTune <- train(x = trainFingerprints, y = log10(trainPermeability),
                 method = "pls",
                 tuneGrid = expand.grid(ncomp = 1:15),
                 trControl = ctrl)

plsTune

plot(plsTune,metric="Rsquared")
```

The above plot indicates that the optimal number of latent variables that maximizes $R^2$ is 9 so the $R^2$ value is .57.

d) Predict the response for the test set. What is the test set estimate of R2?

```{r}
plsTest <- data.frame(Observed=log10(testPermeability),Predicted=predict(plsTune,testFingerprints))
xyplot(Predicted ~ Observed,
       plsTest, panel = function(...) {
         theDots <- list(...)
         panel.xyplot(..., type = c("p", "g","r","smooth"))
         corr <- round(cor(theDots$x, theDots$y), 2)
         panel.text(44, min(theDots$y), paste("corr:", corr))
         },
       ylab = "Predicted",
       xlab = "Observed")
```

The $R^2$ value is 0.541

e) Try building other models discussed in this chapter. Do any have better predictive performance?



f) Would you recommend any of your models to replace the permeability laboratory experiment?

Elastic net solution the best penalized model choice for this problem.

