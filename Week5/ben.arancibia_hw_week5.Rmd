---
title: "Week 5 Homework"
author: "Ben Arancibia"
date: "July 8, 2015"
output: pdf_document
---

```{r}
library(fpp)
```

####HA 6.1

1) Show that a 3Ã—5 MA is equivalent to a 7-term weighted moving average with weights of 0.067, 0.133, 0.200, 0.200, 0.200, 0.133, and 0.067.

First do the 7 term weighted moving average
```{r}
weights <- (c(0.067, 0.133, 0.200, 0.200, 0.200, 0.133, 0.067))

ma7 <- ma(weights, order=7)

ma7
```

Next do the 3x5 MA
```{r}
ma5 <- ma(weights, order=5)
ma3x5 <- ma(ma5, order=3)
```

The reason that 3x5 is equivalent to the 7 term weight moving average is because combinations of moving averages result in weighted moving averages. 

####HA 6.2
The data below represent the monthly sales (in thousands) of product A for a plastics manufacturer for years 1 through 5 (data set plastics).
```{r}
plastics
```

a) Plot the time series of sales of product A. Can you identify seasonal fluctuations and/or a trend?

```{r}
plot(plastics)
```

There is seasonal and upward trend for the product A.

b) Use a classical multiplicative decomposition to calculate the trend-cycle and seasonal indices.

```{r}
decomp <- decompose(plastics, type="multiplicative")

plot(decomp)
```

c) Do the results support the graphical interpretation from part (a)?

Yes the results show there is an unchanged seasonanility (third graphic), but there is a tip in the trend that was not noticed in the first plot.

d) Compute and plot the seasonally adjusted data.

```{r}
sa <- seasadj(decomp)
plot(sa)
```


e) Change one observation to be an outlier (e.g., add 500 to one observation), and recompute the seasonally adjusted data. What is the effect of the outlier?

```{r}
x <- plastics
x[18] <- x[18] + 500
sa2 <- seasadj(stl(x, s.window="periodic"))
sa3 <- seasadj(stl(x, s.window="periodic", robust=TRUE))
plot(sa, col="blue", ylim=range(sa,sa2,sa3))
lines(sa2,col="red")
lines(sa3, col="green")
```

If robust=FALSE, the seasonal adjusted series changes across the whole data set. Notice that there are a lot more differences in the red line compared to the blue line. If robust=TRUE, only the outlying point changes noticeably. There are not that many differences from the blue and green lines compared to the blue line.

f) Does it make any difference if the outlier is near the end rather than in the middle of the time series?

```{r}
#Repeat above function

x <- plastics
x[58] <- x[58] + 500 
sa2 <- seasadj(stl(x, s.window="periodic"))
sa3 <- seasadj(stl(x, s.window="periodic", robust=TRUE))
plot(sa, col="blue", ylim=range(sa,sa2,sa3))
lines(sa2,col="red")
lines(sa3, col="green")
```

Having an outlier near the end gives it more impact.

g) Use a random walk with drift to produce forecasts of the seasonally adjusted data.

```{r}
fc <- stlf(plastics, method="naive")
plot(fc)
```

h) Reseasonalize the results to give forecasts on the original scale.

```{r}
fit <- stl(plastics, s.window="periodic", robust=TRUE)
fitadj <- seasadj(fit)
plot(naive(fitadj), xlab="Plastic Product A",
  main="Naive forecasts of seasonally adjusted data")
```

####HA 7.3

For this exercise, use the quarterly UK passenger vehicle production data from 1977:1--2005:1 (data set ukcars).

a) Plot the data and describe the main features of the series.

```{r}
plot(ukcars)
seasonplot(ukcars,year.labels=TRUE, year.labels.left=TRUE, col=rainbow(30))
monthplot(ukcars)
```

b) Decompose the series using STL and obtain the seasonally adjusted data.

```{r}
decomp <- stl(ukcars, s.window=9, robust=TRUE)
plot(decomp)
```

c) Forecast the next two years of the series using an additive damped trend method applied to the seasonally adjusted data. Then reseasonalize the forecasts. Record the parameters of the method and report the RMSE of the one-step forecasts from your method.

```{r}
sadamped <- seasadj(decomp)
fit <- holt(sadamped, h=8, damped=TRUE)
plot(fit)

accuracy(fit)

fit$model

twoyear <- rep(decomp$time.series[110:113,"seasonal"],2)
fc <- fit$mean + twoyear
plot(ukcars,xlim=c(1980,2007))
lines(fc, col="blue")
```

The parameters show a small beta which means the slope isn't changing much over time and a large alpha meaning the intercept is changing quickly.

RMSE of the model is 23.70867

d) Forecast the next two years of the series using Holt's linear method applied to the seasonally adjusted data. Then reseasonalize the forecasts. Record the parameters of the method and report the RMSE of of the one-step forecasts from your method.

```{r}
sa <- seasadj(decomp)
fit <- holt(sa, h=8)
plot(fit)

accuracy(fit)

fit$model

twoyear <- rep(decomp$time.series[110:113,"seasonal"],2)
fc <- fit$mean + twoyear
plot(ukcars,xlim=c(1980,2007))
lines(fc, col="blue")
```

The parameters are pretty similar compared to the damped time series. The alpha value increased by around .02 and the RMSE of the model increased by .05 (RMSE = 23.75353)

e) Now use ets() to choose a seasonal model for the data.

```{r}
fit <- ets(ukcars)
fit
accuracy(fit)

```

ETS Choose A,N,A an additive seasonal component

f) Compare the RMSE of the fitted model with the RMSE of the model you obtained using an STL decomposition with Holt's method. Which gives the better insample fits?

```{r}
accuracy(ets(ukcars))

sa <- seasadj(decomp)
fit <- holt(sa, h=8)

accuracy(fit)
```

The fitted model using ETS has a higher RMSE value, but not by much. ETS model had 25.24902 and STL Decomp with Holts method had 23.75353

g) Compare the forecasts from the two approaches? Which seems most reasonable?

They are both comparable and don't have that much of a difference. 

####HA 7.4

For this exercise, use the monthly Australian short-term overseas visitors data, May 1985--April 2005. (Data set: visitors.)

a) Make a time plot of your data and describe the main features of the series.

```{r}
plot(visitors)
seasonplot(visitors)
monthplot(visitors)
plot(stl(log(visitors),s.window="periodic",robust=TRUE))
```

The dataset visitors is seasonal data with an increasing trend and increasing seasonal fluctuations. Peak seems to be in December, with smaller peak in July,  andowest numbers of visitors in May. Each month has had similar increases over time. Seasonality looks stable. Trend relatively flat in recent years. Big negative outliers in 2003 (fourth chart).

b) Forecast the next two years using Holt-Winters' multiplicative method.
```{r}
fc <- hw(visitors,seasonal="mult")
plot(fc)
```

c) Why is multiplicative seasonality necessary here?

Multiplicative seasonality is necessary because of the increasing size of the fluctuations and increasing variation with the trend. These two observations were noted above in part (a).

d) Experiment with making the trend exponential and/or damped.

```{r}
fc1 <- hw(visitors,seasonal="mult",exponential=TRUE)
fc2 <- hw(visitors,seasonal="mult",exponential=TRUE, damped=TRUE)
fc3 <- hw(visitors,seasonal="mult",damped=TRUE)

plot(fc1)
plot(fc2)
plot(fc3)
```

e) Compare the RMSE of the one-step forecasts from the various methods. Which do you prefer?

```{r}
accuracy(fc)
accuracy(fc1)
accuracy(fc2)
accuracy(fc3)
```

Additive damped trend (fc3) seems to do best (lowest RMSE) amongst these models. However, it has one more parameter (the damping parameter) than the non-damped version. The damped explonential does better than the non-damped model.

g) Now fit each of the following models to the same data: 

multiplicative Holt-Winters' method
```{r}
fc <- hw(visitors,seasonal="mult")
plot(fc)
```

ETS model
```{r}
etsfit <- ets(visitors)
etsfit
```

additive ETS model applied to a Box-Cox transformed series
```{r}
lambda <- BoxCox.lambda(visitors)
boxcox <- (BoxCox(visitors,lambda))

etsboxcox <- ets(boxcox)

etsboxcox

plot(etsboxcox)
```

seasonal naive method applied to the Box-Cox transformed series
```{r}
snaive <- snaive(boxcox, h=24)

plot(snaive)
```

STL decomposition applied to the Box-Cox transformed data 

```{r}
stlbc <- stl(boxcox, s.window="periodic", robust=TRUE)

boxcox <- seasadj(stlbc)
decompboxcox <- holt(boxcox, h=8)

plot(decompboxcox)
```

ETS model applied to the seasonally adjusted (transformed) data.

```{r}
etsboxcox2 <- ets(boxcox)
etsboxcox2

plot(etsboxcox2)
```

g) For each model, look at the residual diagnostics and compare the forecasts for the next two years. Which do you prefer?

```{r}
accuracy(fc)
accuracy(etsfit)
accuracy(etsboxcox)
accuracy(snaive)
accuracy(decompboxcox)
accuracy(etsboxcox2)
```

The RMSE for the ETS fits are very low, as well as the STL decomp and snaive. This might be as a result of the Box-Cox Transformation. I personally prefer the ETS model because it chooses for you, but the graphics are not as easy to read as compared to the STL decomp and snaive plots. 
